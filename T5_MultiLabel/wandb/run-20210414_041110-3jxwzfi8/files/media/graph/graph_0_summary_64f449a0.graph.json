{"format": "torch", "nodes": [{"name": "shared", "id": 140298741367056, "class_name": "Embedding(32128, 768)", "parameters": [["weight", [32128, 768]]], "output_shape": [[16, 196, 768]], "num_parameters": [24674304]}, {"name": "encoder", "id": 140298799487888, "class_name": "T5Stack(\n  (embed_tokens): Embedding(32128, 768)\n  (block): ModuleList(\n    (0): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n            (relative_attention_bias): Embedding(32, 12)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (1): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (2): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (3): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (4): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (5): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (6): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (7): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (8): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (9): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (10): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (11): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (final_layer_norm): T5LayerNorm()\n  (dropout): Dropout(p=0.1, inplace=False)\n)", "parameters": [["embed_tokens.weight", [32128, 768]], ["block.0.layer.0.SelfAttention.q.weight", [768, 768]], ["block.0.layer.0.SelfAttention.k.weight", [768, 768]], ["block.0.layer.0.SelfAttention.v.weight", [768, 768]], ["block.0.layer.0.SelfAttention.o.weight", [768, 768]], ["block.0.layer.0.SelfAttention.relative_attention_bias.weight", [32, 12]], ["block.0.layer.0.layer_norm.weight", [768]], ["block.0.layer.1.DenseReluDense.wi.weight", [3072, 768]], ["block.0.layer.1.DenseReluDense.wo.weight", [768, 3072]], ["block.0.layer.1.layer_norm.weight", [768]], ["block.1.layer.0.SelfAttention.q.weight", [768, 768]], ["block.1.layer.0.SelfAttention.k.weight", [768, 768]], ["block.1.layer.0.SelfAttention.v.weight", [768, 768]], ["block.1.layer.0.SelfAttention.o.weight", [768, 768]], ["block.1.layer.0.layer_norm.weight", [768]], ["block.1.layer.1.DenseReluDense.wi.weight", [3072, 768]], ["block.1.layer.1.DenseReluDense.wo.weight", [768, 3072]], ["block.1.layer.1.layer_norm.weight", [768]], ["block.2.layer.0.SelfAttention.q.weight", [768, 768]], ["block.2.layer.0.SelfAttention.k.weight", [768, 768]], ["block.2.layer.0.SelfAttention.v.weight", [768, 768]], ["block.2.layer.0.SelfAttention.o.weight", [768, 768]], ["block.2.layer.0.layer_norm.weight", [768]], ["block.2.layer.1.DenseReluDense.wi.weight", [3072, 768]], ["block.2.layer.1.DenseReluDense.wo.weight", [768, 3072]], ["block.2.layer.1.layer_norm.weight", [768]], ["block.3.layer.0.SelfAttention.q.weight", [768, 768]], ["block.3.layer.0.SelfAttention.k.weight", [768, 768]], ["block.3.layer.0.SelfAttention.v.weight", [768, 768]], ["block.3.layer.0.SelfAttention.o.weight", [768, 768]], ["block.3.layer.0.layer_norm.weight", [768]], ["block.3.layer.1.DenseReluDense.wi.weight", [3072, 768]], ["block.3.layer.1.DenseReluDense.wo.weight", [768, 3072]], ["block.3.layer.1.layer_norm.weight", [768]], ["block.4.layer.0.SelfAttention.q.weight", [768, 768]], ["block.4.layer.0.SelfAttention.k.weight", [768, 768]], ["block.4.layer.0.SelfAttention.v.weight", [768, 768]], ["block.4.layer.0.SelfAttention.o.weight", [768, 768]], ["block.4.layer.0.layer_norm.weight", [768]], ["block.4.layer.1.DenseReluDense.wi.weight", [3072, 768]], ["block.4.layer.1.DenseReluDense.wo.weight", [768, 3072]], ["block.4.layer.1.layer_norm.weight", [768]], ["block.5.layer.0.SelfAttention.q.weight", [768, 768]], ["block.5.layer.0.SelfAttention.k.weight", [768, 768]], ["block.5.layer.0.SelfAttention.v.weight", [768, 768]], ["block.5.layer.0.SelfAttention.o.weight", [768, 768]], ["block.5.layer.0.layer_norm.weight", [768]], ["block.5.layer.1.DenseReluDense.wi.weight", [3072, 768]], ["block.5.layer.1.DenseReluDense.wo.weight", [768, 3072]], ["block.5.layer.1.layer_norm.weight", [768]], ["block.6.layer.0.SelfAttention.q.weight", [768, 768]], ["block.6.layer.0.SelfAttention.k.weight", [768, 768]], ["block.6.layer.0.SelfAttention.v.weight", [768, 768]], ["block.6.layer.0.SelfAttention.o.weight", [768, 768]], ["block.6.layer.0.layer_norm.weight", [768]], ["block.6.layer.1.DenseReluDense.wi.weight", [3072, 768]], ["block.6.layer.1.DenseReluDense.wo.weight", [768, 3072]], ["block.6.layer.1.layer_norm.weight", [768]], ["block.7.layer.0.SelfAttention.q.weight", [768, 768]], ["block.7.layer.0.SelfAttention.k.weight", [768, 768]], ["block.7.layer.0.SelfAttention.v.weight", [768, 768]], ["block.7.layer.0.SelfAttention.o.weight", [768, 768]], ["block.7.layer.0.layer_norm.weight", [768]], ["block.7.layer.1.DenseReluDense.wi.weight", [3072, 768]], ["block.7.layer.1.DenseReluDense.wo.weight", [768, 3072]], ["block.7.layer.1.layer_norm.weight", [768]], ["block.8.layer.0.SelfAttention.q.weight", [768, 768]], ["block.8.layer.0.SelfAttention.k.weight", [768, 768]], ["block.8.layer.0.SelfAttention.v.weight", [768, 768]], ["block.8.layer.0.SelfAttention.o.weight", [768, 768]], ["block.8.layer.0.layer_norm.weight", [768]], ["block.8.layer.1.DenseReluDense.wi.weight", [3072, 768]], ["block.8.layer.1.DenseReluDense.wo.weight", [768, 3072]], ["block.8.layer.1.layer_norm.weight", [768]], ["block.9.layer.0.SelfAttention.q.weight", [768, 768]], ["block.9.layer.0.SelfAttention.k.weight", [768, 768]], ["block.9.layer.0.SelfAttention.v.weight", [768, 768]], ["block.9.layer.0.SelfAttention.o.weight", [768, 768]], ["block.9.layer.0.layer_norm.weight", [768]], ["block.9.layer.1.DenseReluDense.wi.weight", [3072, 768]], ["block.9.layer.1.DenseReluDense.wo.weight", [768, 3072]], ["block.9.layer.1.layer_norm.weight", [768]], ["block.10.layer.0.SelfAttention.q.weight", [768, 768]], ["block.10.layer.0.SelfAttention.k.weight", [768, 768]], ["block.10.layer.0.SelfAttention.v.weight", [768, 768]], ["block.10.layer.0.SelfAttention.o.weight", [768, 768]], ["block.10.layer.0.layer_norm.weight", [768]], ["block.10.layer.1.DenseReluDense.wi.weight", [3072, 768]], ["block.10.layer.1.DenseReluDense.wo.weight", [768, 3072]], ["block.10.layer.1.layer_norm.weight", [768]], ["block.11.layer.0.SelfAttention.q.weight", [768, 768]], ["block.11.layer.0.SelfAttention.k.weight", [768, 768]], ["block.11.layer.0.SelfAttention.v.weight", [768, 768]], ["block.11.layer.0.SelfAttention.o.weight", [768, 768]], ["block.11.layer.0.layer_norm.weight", [768]], ["block.11.layer.1.DenseReluDense.wi.weight", [3072, 768]], ["block.11.layer.1.DenseReluDense.wo.weight", [768, 3072]], ["block.11.layer.1.layer_norm.weight", [768]], ["final_layer_norm.weight", [768]]], "output_shape": [[[[0], [0], [0], [0], [0], [0], [0], [0], 0, [0], [0], 0, 0, 0, 0, 0, 0]]], "num_parameters": [24674304, 589824, 589824, 589824, 589824, 384, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 768]}, {"name": "decoder", "id": 140298555713616, "class_name": "T5Stack(\n  (embed_tokens): Embedding(32128, 768)\n  (block): ModuleList(\n    (0): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n            (relative_attention_bias): Embedding(32, 12)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (1): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (2): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (3): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (4): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (5): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (6): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (7): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (8): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (9): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (10): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (11): T5Block(\n      (layer): ModuleList(\n        (0): T5LayerSelfAttention(\n          (SelfAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (1): T5LayerCrossAttention(\n          (EncDecAttention): T5Attention(\n            (q): Linear(in_features=768, out_features=768, bias=False)\n            (k): Linear(in_features=768, out_features=768, bias=False)\n            (v): Linear(in_features=768, out_features=768, bias=False)\n            (o): Linear(in_features=768, out_features=768, bias=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (2): T5LayerFF(\n          (DenseReluDense): T5DenseReluDense(\n            (wi): Linear(in_features=768, out_features=3072, bias=False)\n            (wo): Linear(in_features=3072, out_features=768, bias=False)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (final_layer_norm): T5LayerNorm()\n  (dropout): Dropout(p=0.1, inplace=False)\n)", "parameters": [["embed_tokens.weight", [32128, 768]], ["block.0.layer.0.SelfAttention.q.weight", [768, 768]], ["block.0.layer.0.SelfAttention.k.weight", [768, 768]], ["block.0.layer.0.SelfAttention.v.weight", [768, 768]], ["block.0.layer.0.SelfAttention.o.weight", [768, 768]], ["block.0.layer.0.SelfAttention.relative_attention_bias.weight", [32, 12]], ["block.0.layer.0.layer_norm.weight", [768]], ["block.0.layer.1.EncDecAttention.q.weight", [768, 768]], ["block.0.layer.1.EncDecAttention.k.weight", [768, 768]], ["block.0.layer.1.EncDecAttention.v.weight", [768, 768]], ["block.0.layer.1.EncDecAttention.o.weight", [768, 768]], ["block.0.layer.1.layer_norm.weight", [768]], ["block.0.layer.2.DenseReluDense.wi.weight", [3072, 768]], ["block.0.layer.2.DenseReluDense.wo.weight", [768, 3072]], ["block.0.layer.2.layer_norm.weight", [768]], ["block.1.layer.0.SelfAttention.q.weight", [768, 768]], ["block.1.layer.0.SelfAttention.k.weight", [768, 768]], ["block.1.layer.0.SelfAttention.v.weight", [768, 768]], ["block.1.layer.0.SelfAttention.o.weight", [768, 768]], ["block.1.layer.0.layer_norm.weight", [768]], ["block.1.layer.1.EncDecAttention.q.weight", [768, 768]], ["block.1.layer.1.EncDecAttention.k.weight", [768, 768]], ["block.1.layer.1.EncDecAttention.v.weight", [768, 768]], ["block.1.layer.1.EncDecAttention.o.weight", [768, 768]], ["block.1.layer.1.layer_norm.weight", [768]], ["block.1.layer.2.DenseReluDense.wi.weight", [3072, 768]], ["block.1.layer.2.DenseReluDense.wo.weight", [768, 3072]], ["block.1.layer.2.layer_norm.weight", [768]], ["block.2.layer.0.SelfAttention.q.weight", [768, 768]], ["block.2.layer.0.SelfAttention.k.weight", [768, 768]], ["block.2.layer.0.SelfAttention.v.weight", [768, 768]], ["block.2.layer.0.SelfAttention.o.weight", [768, 768]], ["block.2.layer.0.layer_norm.weight", [768]], ["block.2.layer.1.EncDecAttention.q.weight", [768, 768]], ["block.2.layer.1.EncDecAttention.k.weight", [768, 768]], ["block.2.layer.1.EncDecAttention.v.weight", [768, 768]], ["block.2.layer.1.EncDecAttention.o.weight", [768, 768]], ["block.2.layer.1.layer_norm.weight", [768]], ["block.2.layer.2.DenseReluDense.wi.weight", [3072, 768]], ["block.2.layer.2.DenseReluDense.wo.weight", [768, 3072]], ["block.2.layer.2.layer_norm.weight", [768]], ["block.3.layer.0.SelfAttention.q.weight", [768, 768]], ["block.3.layer.0.SelfAttention.k.weight", [768, 768]], ["block.3.layer.0.SelfAttention.v.weight", [768, 768]], ["block.3.layer.0.SelfAttention.o.weight", [768, 768]], ["block.3.layer.0.layer_norm.weight", [768]], ["block.3.layer.1.EncDecAttention.q.weight", [768, 768]], ["block.3.layer.1.EncDecAttention.k.weight", [768, 768]], ["block.3.layer.1.EncDecAttention.v.weight", [768, 768]], ["block.3.layer.1.EncDecAttention.o.weight", [768, 768]], ["block.3.layer.1.layer_norm.weight", [768]], ["block.3.layer.2.DenseReluDense.wi.weight", [3072, 768]], ["block.3.layer.2.DenseReluDense.wo.weight", [768, 3072]], ["block.3.layer.2.layer_norm.weight", [768]], ["block.4.layer.0.SelfAttention.q.weight", [768, 768]], ["block.4.layer.0.SelfAttention.k.weight", [768, 768]], ["block.4.layer.0.SelfAttention.v.weight", [768, 768]], ["block.4.layer.0.SelfAttention.o.weight", [768, 768]], ["block.4.layer.0.layer_norm.weight", [768]], ["block.4.layer.1.EncDecAttention.q.weight", [768, 768]], ["block.4.layer.1.EncDecAttention.k.weight", [768, 768]], ["block.4.layer.1.EncDecAttention.v.weight", [768, 768]], ["block.4.layer.1.EncDecAttention.o.weight", [768, 768]], ["block.4.layer.1.layer_norm.weight", [768]], ["block.4.layer.2.DenseReluDense.wi.weight", [3072, 768]], ["block.4.layer.2.DenseReluDense.wo.weight", [768, 3072]], ["block.4.layer.2.layer_norm.weight", [768]], ["block.5.layer.0.SelfAttention.q.weight", [768, 768]], ["block.5.layer.0.SelfAttention.k.weight", [768, 768]], ["block.5.layer.0.SelfAttention.v.weight", [768, 768]], ["block.5.layer.0.SelfAttention.o.weight", [768, 768]], ["block.5.layer.0.layer_norm.weight", [768]], ["block.5.layer.1.EncDecAttention.q.weight", [768, 768]], ["block.5.layer.1.EncDecAttention.k.weight", [768, 768]], ["block.5.layer.1.EncDecAttention.v.weight", [768, 768]], ["block.5.layer.1.EncDecAttention.o.weight", [768, 768]], ["block.5.layer.1.layer_norm.weight", [768]], ["block.5.layer.2.DenseReluDense.wi.weight", [3072, 768]], ["block.5.layer.2.DenseReluDense.wo.weight", [768, 3072]], ["block.5.layer.2.layer_norm.weight", [768]], ["block.6.layer.0.SelfAttention.q.weight", [768, 768]], ["block.6.layer.0.SelfAttention.k.weight", [768, 768]], ["block.6.layer.0.SelfAttention.v.weight", [768, 768]], ["block.6.layer.0.SelfAttention.o.weight", [768, 768]], ["block.6.layer.0.layer_norm.weight", [768]], ["block.6.layer.1.EncDecAttention.q.weight", [768, 768]], ["block.6.layer.1.EncDecAttention.k.weight", [768, 768]], ["block.6.layer.1.EncDecAttention.v.weight", [768, 768]], ["block.6.layer.1.EncDecAttention.o.weight", [768, 768]], ["block.6.layer.1.layer_norm.weight", [768]], ["block.6.layer.2.DenseReluDense.wi.weight", [3072, 768]], ["block.6.layer.2.DenseReluDense.wo.weight", [768, 3072]], ["block.6.layer.2.layer_norm.weight", [768]], ["block.7.layer.0.SelfAttention.q.weight", [768, 768]], ["block.7.layer.0.SelfAttention.k.weight", [768, 768]], ["block.7.layer.0.SelfAttention.v.weight", [768, 768]], ["block.7.layer.0.SelfAttention.o.weight", [768, 768]], ["block.7.layer.0.layer_norm.weight", [768]], ["block.7.layer.1.EncDecAttention.q.weight", [768, 768]], ["block.7.layer.1.EncDecAttention.k.weight", [768, 768]], ["block.7.layer.1.EncDecAttention.v.weight", [768, 768]], ["block.7.layer.1.EncDecAttention.o.weight", [768, 768]], ["block.7.layer.1.layer_norm.weight", [768]], ["block.7.layer.2.DenseReluDense.wi.weight", [3072, 768]], ["block.7.layer.2.DenseReluDense.wo.weight", [768, 3072]], ["block.7.layer.2.layer_norm.weight", [768]], ["block.8.layer.0.SelfAttention.q.weight", [768, 768]], ["block.8.layer.0.SelfAttention.k.weight", [768, 768]], ["block.8.layer.0.SelfAttention.v.weight", [768, 768]], ["block.8.layer.0.SelfAttention.o.weight", [768, 768]], ["block.8.layer.0.layer_norm.weight", [768]], ["block.8.layer.1.EncDecAttention.q.weight", [768, 768]], ["block.8.layer.1.EncDecAttention.k.weight", [768, 768]], ["block.8.layer.1.EncDecAttention.v.weight", [768, 768]], ["block.8.layer.1.EncDecAttention.o.weight", [768, 768]], ["block.8.layer.1.layer_norm.weight", [768]], ["block.8.layer.2.DenseReluDense.wi.weight", [3072, 768]], ["block.8.layer.2.DenseReluDense.wo.weight", [768, 3072]], ["block.8.layer.2.layer_norm.weight", [768]], ["block.9.layer.0.SelfAttention.q.weight", [768, 768]], ["block.9.layer.0.SelfAttention.k.weight", [768, 768]], ["block.9.layer.0.SelfAttention.v.weight", [768, 768]], ["block.9.layer.0.SelfAttention.o.weight", [768, 768]], ["block.9.layer.0.layer_norm.weight", [768]], ["block.9.layer.1.EncDecAttention.q.weight", [768, 768]], ["block.9.layer.1.EncDecAttention.k.weight", [768, 768]], ["block.9.layer.1.EncDecAttention.v.weight", [768, 768]], ["block.9.layer.1.EncDecAttention.o.weight", [768, 768]], ["block.9.layer.1.layer_norm.weight", [768]], ["block.9.layer.2.DenseReluDense.wi.weight", [3072, 768]], ["block.9.layer.2.DenseReluDense.wo.weight", [768, 3072]], ["block.9.layer.2.layer_norm.weight", [768]], ["block.10.layer.0.SelfAttention.q.weight", [768, 768]], ["block.10.layer.0.SelfAttention.k.weight", [768, 768]], ["block.10.layer.0.SelfAttention.v.weight", [768, 768]], ["block.10.layer.0.SelfAttention.o.weight", [768, 768]], ["block.10.layer.0.layer_norm.weight", [768]], ["block.10.layer.1.EncDecAttention.q.weight", [768, 768]], ["block.10.layer.1.EncDecAttention.k.weight", [768, 768]], ["block.10.layer.1.EncDecAttention.v.weight", [768, 768]], ["block.10.layer.1.EncDecAttention.o.weight", [768, 768]], ["block.10.layer.1.layer_norm.weight", [768]], ["block.10.layer.2.DenseReluDense.wi.weight", [3072, 768]], ["block.10.layer.2.DenseReluDense.wo.weight", [768, 3072]], ["block.10.layer.2.layer_norm.weight", [768]], ["block.11.layer.0.SelfAttention.q.weight", [768, 768]], ["block.11.layer.0.SelfAttention.k.weight", [768, 768]], ["block.11.layer.0.SelfAttention.v.weight", [768, 768]], ["block.11.layer.0.SelfAttention.o.weight", [768, 768]], ["block.11.layer.0.layer_norm.weight", [768]], ["block.11.layer.1.EncDecAttention.q.weight", [768, 768]], ["block.11.layer.1.EncDecAttention.k.weight", [768, 768]], ["block.11.layer.1.EncDecAttention.v.weight", [768, 768]], ["block.11.layer.1.EncDecAttention.o.weight", [768, 768]], ["block.11.layer.1.layer_norm.weight", [768]], ["block.11.layer.2.DenseReluDense.wi.weight", [3072, 768]], ["block.11.layer.2.DenseReluDense.wo.weight", [768, 3072]], ["block.11.layer.2.layer_norm.weight", [768]], ["final_layer_norm.weight", [768]]], "output_shape": [[[[0], [0], [0], [0], [0], [0], [0], [0], 0, [0], [0], 0, 0, 0, 0, 0, 0], [[0], 0, 0, 0, 0, [0], 0, [0], 0, [0], 0, 0, [0], 0, 0]]], "num_parameters": [24674304, 589824, 589824, 589824, 589824, 384, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 589824, 589824, 589824, 589824, 768, 589824, 589824, 589824, 589824, 768, 2359296, 2359296, 768, 768]}, {"name": "lm_head", "id": 140298733287888, "class_name": "Linear(in_features=768, out_features=32128, bias=False)", "parameters": [["weight", [32128, 768]]], "output_shape": [[16, 196, 32128]], "num_parameters": [24674304]}], "edges": []}